{"cells":[{"cell_type":"code","source":["##########################################################################################################\n# VERSION  \tDESARROLLADOR             FECHA        DESCRIPCION\n# -------------------------------------------------------------\n#  1        Walter Albites Azarte     15/06/2020   Curso Apache Spark con Databricks\n##########################################################################################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e9ef7f9-1a12-41c5-abcd-f4e4793e8f64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://docs.databricks.com/_static/databricks-logo-mobile.png\" style=\"float: left: margin: 20px\"/>\n# Curso Apache Spark con Databricks\n* Databricks es una plataforma de análisis basada en Apache Spark optimizada para la plataforma de servicios en la nube AWS y Azure. Diseñado con los fundadores de Apache Spark, Databricks está integrado con Azure y AWS para proporcionar configuración con un solo clic.\n* Si juntamos estos dos conceptos principales, que son Spark y Notebooks, tenemos un 75% de la definición de Databricks.\n* Pensemos en Databricks como un espacio de trabajo, Workspace,  basado en Apache Spark, que permite colaborar a científicos de datos e ingenieros de datos en diferentes clusters mediante el desarrollo de Notebooks y bajo el soporte de un Runtime basado en Apache Spark donde se ejecutan todas las operaciones.\n* Costos https://azure.microsoft.com/es-es/pricing/calculator/\n\n## Objetivo del Curso:\n* El objetivo perseguido es que el alumno aprenda a poder trabajar los notebooks para poder aplicar ETL y ELT en cluster Nube.\n\n## Temario:\n* Introducción a Databricks\n* Componentes de Databricks\n* Comparar Cluster Nube Databricks VS Cluster On-Premise\n* Databricks y su relación con otros componentes\n* Que es Spark\n* Spark VS PySpark\n* Que es Cluster\n* Arquetipo de un Datalake capas RDV,UDV,DDV\n* Crear una cuenta en el community Databricks\n* Crear Cluster en Databricks\n* Administracion de Cluster\n* Load archivo csv a DBFS\n* Como usar los notebooks en databricks\n* Crear un Notebook Lenguaje Python.\n* Importar Librerias Necesarias pyspark y pandas\n* Comandos magicos para crear editar mover directorios y archivos\n* explicacion pipeline ETL\n* Forma Leer en un Dataframe de Pyspark un CSV\n* Forma Leer en un Dataframe de Pyspark un CSV, no lo lee bien falta un cero a proposito para aplicar una transformacion\n* Crear un UDF función definida por el usuario para completar el cero adelante de una columna\n* Aplicar la transformacion de la columna aplicando UDF\n* Crear la tabla ubigeo en la base de datos databricks\n* Leer las tablas de base de datos de Databricks con SQL\n* Agregar una nueva columna en un marco de datos PySpark Dataframe\n* Eliminar una columna en un marco de datos PySpark Dataframe\n* Agregar una nueva columna en las tablas de base de datos de Databricks\n* Eliminar una columna en las tablas de base de datos de Databricks\n* Aplicando pandas en Databricks\n\n## Caso de Uso:\n* Cargar csv para almacenarlas en tablas de base de datos Databricks.\n - Cargar un archivo csv con datos de ubigeo en el dbfs y poblarlo en una tabla de base de datos de databricks de tipo parquet para realizar la consulta y exportación de la data.\n\n\n* <a href=\"https://webxstudio.net/cursos/databricks/basico/1-Introduccion.html\">1 - Introduccion a Databricks</a>\n\n\n## Pasos:\n1. Iniciar el Cluster.\n2. Cargar el archivo csv al databricks.\n - <a href=\"https://webxstudio.net/cursos/databricks/basico/2-Load-File.html\">2 - Load File</a>\n3. Crear un Notebook Lenguaje Python.\n4. Importar las librerías necesarias ejemplo pyspark,pandas.\n5. Leer el archivo csv del dbfs.\n - <a href=\"https://webxstudio.net/cursos/databricks/basico/3-ETL-Ubigeo.html\">3 - ETL Ubigeo</a>\n6. Crear la tabla en la base de datos con la misma estructura del csv en formato Parquet.\n7. Transformar y limpiar la data para poder poblar la tabla creada. \n8. Consultar la tabla poblada con sintaxis SQL.\n - <a href=\"https://webxstudio.net/cursos/databricks/basico/3-ETL-Ubigeo.html\">3 - ETL Ubigeo</a>\n \n## Prerrequisitos:\n* El participante deberá tener una cuenta creada\nhttps://community.cloud.databricks.com/login.html\n* Conocimientos Basicos de Pyspark o Spark\n* Descargar el archivo ubigeo csv\n<a href=\"https://webxstudio.net/cursos/databricks/basico/ubigeo.csv\">ubigeo.csv</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d21c5954-5584-4bab-9bab-a8ee8c28ccde"}}},{"cell_type":"markdown","source":["### Comandos mágicos auxiliares para celdas\n\n* %sh: Permite ejecutar código de Shell en el Bloc de notas. Para que se produzca un error en la celda si el comando de Shell tiene un estado de -e salida distinto de cero, agregue la opción. Este comando solo se ejecuta en el controlador de Apache Spark y no en los trabajos. Para ejecutar un comando de Shell en todos los nodos, use un script init.\n* %fs: Permite usar dbutils comandos del sistema de archivos.\n* %md: Permite incluir varios tipos de documentación, como texto, imágenes, fórmulas y ecuaciones matemáticas.\n* %sql: Sintaxis SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d86d74c2-3d62-427c-a96e-53f122936747"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1-Introduccion","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":875907104918272}},"nbformat":4,"nbformat_minor":0}
